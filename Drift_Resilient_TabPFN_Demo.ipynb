{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "C1A93ZThmGyZ",
        "KVZBlvw_u3PE",
        "bZV28AJCtDBy",
        "nU5FxwrJEeX9",
        "wb4W8Z9GtKZY",
        "IxdSrFVpu67d",
        "UeFNUrWa4qLz",
        "ubhCj791v71L",
        "qFWQ_SAau-lq",
        "j_CFFQZBOD4l"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Drift-Resilient TabPFN"
      ],
      "metadata": {
        "id": "E4qwHqTadpo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation (this takes about 5 minutes)"
      ],
      "metadata": {
        "id": "VddKrnRLfkWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JazZbrdNdfyM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "username = \"automl\"\n",
        "repo = \"Drift-Resilient_TabPFN\"\n",
        "\n",
        "# Uninstall pre-installed torch, torchaudio, torchvision, and torchtext to\n",
        "# prevent conflicts\n",
        "!pip uninstall -y torch torchaudio torchvision torchtext numpy tsfresh transformers sentence-transformers peft\n",
        "# Clone our Package, including code to run the experiments.\n",
        "!pip install git+https://github.com/{username}/{repo}.git\n",
        "\n",
        "# Due to a numpy revert to <2.0.0 we have to restart the runtime before we move on\n",
        "# See https://github.com/googlecolab/colabtools/issues/5238\n",
        "print('Stopping runtime due to numpy downgrade!')\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Check\n",
        "\n",
        "To get fast training/inference times, enable GPU processing for this notebook by navigating to Edit â†’ Notebook Settings and selecting GPU as the Hardware accelerator."
      ],
      "metadata": {
        "id": "UmSb0JBVkr2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeWarning(\n",
        "        \"No GPU was found. Change the notebook settings for faster training/inference as described above.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "8VksUx2VlS5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Helpers\n",
        "\n",
        "Helper function used throughout the notebook. Execute this cell before moving on to the sections below."
      ],
      "metadata": {
        "id": "Abm2aynGSK4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import contextmanager, nullcontext\n",
        "import functools\n",
        "import warnings\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "import requests\n",
        "\n",
        "@contextmanager\n",
        "def temporary_no_ssl_verify():\n",
        "    \"\"\"\n",
        "    Temporarily monkey-patch requests so all HTTPS calls use verify=False.\n",
        "    Restores the original behaviour on exit.\n",
        "    \"\"\"\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", InsecureRequestWarning)\n",
        "\n",
        "        _orig_request = requests.Session.request\n",
        "        requests.Session.request = functools.partialmethod(_orig_request, verify=False)\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            requests.Session.request = _orig_request"
      ],
      "metadata": {
        "id": "H8-ZPTbRSH8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models\n",
        "\n",
        "In this section, we will first load all the pre-trained models that we will use throughout this demo."
      ],
      "metadata": {
        "id": "-Q-cLqZntLTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import resources\n",
        "\n",
        "import tabpfn\n",
        "from tabpfn.scripts.tabular_baselines import transformer_metric\n",
        "from tabpfn.best_models import get_best_tabpfn, TabPFNModelPathsConfig\n",
        "\n",
        "# Get the library path for the tabpfn package\n",
        "libpath = resources.files(tabpfn)\n",
        "\n",
        "\n",
        "# Helper function to load each pre-trained model with the corresponding configuration.\n",
        "def get_model(task_type, model_path, model_type):\n",
        "    model_path_config = TabPFNModelPathsConfig(\n",
        "        paths=[f\"{libpath}/model_cache/{model_path}.cpkt\"], task_type=task_type\n",
        "    )\n",
        "\n",
        "    model = get_best_tabpfn(\n",
        "        task_type=task_type,\n",
        "        model_type=model_type,\n",
        "        paths_config=model_path_config,\n",
        "        debug=False,\n",
        "        device=\"auto\"\n",
        "    )\n",
        "    model.show_progress = False\n",
        "    model.seed = 1\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "task_type = \"dist_shift_multiclass\"\n",
        "\n",
        "models_to_load = [\n",
        "    (\"tabpfn_dist_model_1\", \"best_dist\"),\n",
        "    (\"tabpfn_dist_model_2\", \"best_dist\"),\n",
        "    (\"tabpfn_dist_model_3\", \"best_dist\"),\n",
        "    (\"tabpfn_dist_ablation_no_t2v_model_1\", \"best_dist\"),\n",
        "    (\"tabpfn_base_model_1\", \"best_base\"),\n",
        "    (\"tabpfn_base_model_2\", \"best_base\"),\n",
        "    (\"tabpfn_base_model_3\", \"best_base\"),\n",
        "]\n",
        "\n",
        "# Load each model\n",
        "models = {\n",
        "    model_name: get_model(task_type, model_name, model_type)\n",
        "    for model_name, model_type in models_to_load\n",
        "}"
      ],
      "metadata": {
        "id": "K19jIFI1tKl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display one of the loaded distribution shift models\n",
        "models[\"tabpfn_dist_model_1\"]"
      ],
      "metadata": {
        "id": "gcvz5LaDuTyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Drift-Resilient TabPFN\n",
        "\n",
        "This section briefly shows how to interact with the Sklearn interface of our models."
      ],
      "metadata": {
        "id": "C1A93ZThmGyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tabpfn.datasets.dist_shift_datasets import get_rotated_moons_drain\n",
        "\n",
        "# Get one of our pre-trained models\n",
        "clf = models[\"tabpfn_dist_model_1\"]\n",
        "\n",
        "# Load the Rotated Two Moons dataset\n",
        "dataset = get_rotated_moons_drain()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test, dist_shift_domain_train, dist_shift_domain_test = train_test_split(\n",
        "    dataset.x, dataset.y, dataset.dist_shift_domain, test_size=0.50, shuffle=False, random_state=42)\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(X_train, y_train, additional_x={\"dist_shift_domain\": dist_shift_domain_train})\n",
        "\n",
        "# Predict probabilities on the test data\n",
        "preds = clf.predict_proba(X_test, additional_x={\"dist_shift_domain\": dist_shift_domain_test})\n",
        "\n",
        "# Get the predicted classes\n",
        "y_eval = np.argmax(preds, axis=1)\n",
        "\n",
        "# Print ROC AUC and accuracy scores\n",
        "print(\"\")\n",
        "print('ROC AUC: ', roc_auc_score(y_test, preds[:,1], multi_class='ovo'), 'Accuracy', accuracy_score(y_test, y_eval))\n"
      ],
      "metadata": {
        "id": "fEazdm4VmL0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative Analysis\n",
        "\n",
        "Here you can visualise the shifts in our synthetic 2D datasets and analyze the decision boundaries of TabPFN$_\\text{dist}$ compared to TabPFN$_\\text{base}$."
      ],
      "metadata": {
        "id": "KVZBlvw_u3PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Setup\n",
        "\n",
        "Execute this cell before moving on to the sections below."
      ],
      "metadata": {
        "id": "bZV28AJCtDBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabpfn.datasets import get_benchmark_for_task\n",
        "from tabpfn.utils import default_task_settings\n",
        "\n",
        "max_samples, max_features, max_times, max_classes = default_task_settings()\n",
        "\n",
        "datasets_dict = {}\n",
        "\n",
        "# In case the dataset folktables can't be loaded from US census with a certificate error, activate the following context manager to ignore it:\n",
        "with temporary_no_ssl_verify():\n",
        "# with nullcontext():\n",
        "    # Load the validation and test datasets.\n",
        "    for split in [\"valid\", \"test\"]:\n",
        "        datasets, _ = get_benchmark_for_task(\n",
        "            task_type=task_type,\n",
        "            split=split,\n",
        "            max_samples=max_samples,\n",
        "            max_features=max_features,\n",
        "            max_classes=max_classes,\n",
        "            return_as_lists=False,\n",
        "        )\n",
        "\n",
        "        datasets_dict[split] = datasets\n",
        "\n",
        "# Map the names of our models to a display string\n",
        "name_mapping = {\n",
        "    \"tabpfn_dist_model_1\": \"TabPFN$_{\\\\mathrm{dist}_1}$\",\n",
        "    \"tabpfn_dist_model_2\": \"TabPFN$_{\\\\mathrm{dist}_2}$\",\n",
        "    \"tabpfn_dist_model_3\": \"TabPFN$_{\\\\mathrm{dist}_3}$\",\n",
        "    \"tabpfn_dist_ablation_no_t2v_model_1\": \"TabPFN$_{\\\\mathrm{no t2v}}$\",\n",
        "    \"tabpfn_base_model_1\": \"TabPFN$_{\\\\mathrm{base}_1}$\",\n",
        "    \"tabpfn_base_model_2\": \"TabPFN$_{\\\\mathrm{base}_2}$\",\n",
        "    \"tabpfn_base_model_3\": \"TabPFN$_{\\\\mathrm{base}_3}$\",\n",
        "}"
      ],
      "metadata": {
        "id": "Kcy6YoVXtJ0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Plots\n",
        "\n",
        "Plot the datapoints of our 2D datasets across domains."
      ],
      "metadata": {
        "id": "nU5FxwrJEeX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the available datasets to plot decision boundaries (e.g. that have 2 features)\n",
        "print(\"Validation Datasets with 2 Features:\")\n",
        "for i, dataset in enumerate(datasets_dict[\"valid\"]):\n",
        "    if dataset.x.shape[1] == 2:\n",
        "        print(f\"{i}: {dataset}\")\n",
        "\n",
        "print(\"Test Datasets with 2 Features:\")\n",
        "for i, dataset in enumerate(datasets_dict[\"test\"]):\n",
        "    if dataset.x.shape[1] == 2:\n",
        "        print(f\"{i}: {dataset}\")"
      ],
      "metadata": {
        "id": "7jFYgBIiEgJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rotating Two Moons\n",
        "datasets_dict[\"test\"][0].plot(animate=True)"
      ],
      "metadata": {
        "id": "MpNG6L2ZEhlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intersecting Blobs Dataset\n",
        "datasets_dict[\"test\"][2].plot(animate=True)"
      ],
      "metadata": {
        "id": "ZuMHJ2jdEjmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Boundaries\n",
        "\n",
        "Plot and compare the decision boundaries of our pre-trained models given a range of training domains on the unseen out-of-distribution test domains."
      ],
      "metadata": {
        "id": "wb4W8Z9GtKZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the available datasets to plot decision boundaries (e.g. that have 2 features)\n",
        "print(\"Validation Datasets with 2 Features:\")\n",
        "for i, dataset in enumerate(datasets_dict[\"valid\"]):\n",
        "    if dataset.x.shape[1] == 2:\n",
        "        print(f\"{i}: {dataset}\")\n",
        "\n",
        "print(\"Test Datasets with 2 Features:\")\n",
        "for i, dataset in enumerate(datasets_dict[\"test\"]):\n",
        "    if dataset.x.shape[1] == 2:\n",
        "        print(f\"{i}: {dataset}\")"
      ],
      "metadata": {
        "id": "px0QJB4F7g98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "from tabpfn.scripts.decision_boundary import plot_decision_boundary\n",
        "\n",
        "# Select a subset of the models to compare here\n",
        "models_to_eval = {\n",
        "    name: model\n",
        "    for name, model in models.items()\n",
        "    if name in {\"tabpfn_dist_model_1\", \"tabpfn_base_model_1\"}\n",
        "}\n",
        "\n",
        "# Check that N_ensemble_configurations is not None, since this applies a different number of ensemble configurations\n",
        "# for the grid to be predicted than for the foreground samples of the dataset visualized, leading to a mismatch\n",
        "# in the plots.\n",
        "assert all(model.N_ensemble_configurations is not None for model in models_to_eval.values()), \"Detected N_ensemble_configurations=None in one of the models, aborting.\"\n",
        "\n",
        "# Select the index of the dataset to plot according to the cell above\n",
        "current_dataset = datasets_dict[\"test\"][2]\n",
        "\n",
        "# Set the number of domains to train with\n",
        "num_train_domains = 4\n",
        "\n",
        "# Set the number of domains to predict into the future, depending on the dataset\n",
        "max_predict_domains = 3\n",
        "\n",
        "# Define the plot\n",
        "plt.rcParams.update({\"font.size\": 16})\n",
        "plt.rcParams[\"axes.linewidth\"] = 0.5\n",
        "fig, axs = plt.subplots(\n",
        "    len(models_to_eval),\n",
        "    max_predict_domains,\n",
        "    figsize=(6 * (max_predict_domains + 0.6), 4 * (len(models_to_eval))),\n",
        ")\n",
        "fig.set_dpi(300)\n",
        "fig.subplots_adjust(\n",
        "    bottom=0.05, top=0.95, left=0.1, right=0.8, wspace=0.03, hspace=0.03\n",
        ")\n",
        "if axs.ndim == 1:\n",
        "    axs = axs.reshape(1, -1)\n",
        "# Add the colorbar as a separate axis in the end\n",
        "cbar_ax = fig.add_axes([0.82, 0.1, 0.03, 0.6])\n",
        "legend_ax = fig.add_axes([0.81, 0.75, 0.03, 0.15])\n",
        "\n",
        "def process_model(model, label, k):\n",
        "    num_classes = current_dataset.y.unique().shape[0]\n",
        "\n",
        "    # Split the data into the train and id/ood test sets.\n",
        "    train_ds, test_portions = current_dataset.generate_valid_split(\n",
        "        test_set_start_index=num_train_domains, num_predict_domains=max_predict_domains\n",
        "    )\n",
        "    ood_ds, id_ds = test_portions[\"ood\"], test_portions[\"id\"]\n",
        "\n",
        "    # To be comparable to DRAIN and GI we add the id test set back to the training data.\n",
        "    train_ds.x = torch.concat([train_ds.x, id_ds.x], dim=0)\n",
        "    train_ds.dist_shift_domain = torch.concat(\n",
        "        [train_ds.dist_shift_domain, id_ds.dist_shift_domain], dim=0\n",
        "    )\n",
        "    train_ds.y = torch.concat([train_ds.y, id_ds.y], dim=0)\n",
        "\n",
        "    # Fit the train set and predict the ood test set.\n",
        "    model = model.fit(\n",
        "        train_ds.x,\n",
        "        train_ds.y,\n",
        "        additional_x={\"dist_shift_domain\": train_ds.dist_shift_domain},\n",
        "    )\n",
        "    pred = model.predict(\n",
        "        ood_ds.x, additional_x={\"dist_shift_domain\": ood_ds.dist_shift_domain}\n",
        "    )\n",
        "\n",
        "    # Get the unique domains we predicted\n",
        "    unique_vals = torch.unique(ood_ds.dist_shift_domain)\n",
        "\n",
        "    # Plot the decision boundary for each domain separately and display it alongside\n",
        "    # the predicted samples for each domain.\n",
        "    for j, domain in enumerate(unique_vals[:max_predict_domains]):\n",
        "        # Get the axis to plot into\n",
        "        ax = axs[k, j]\n",
        "\n",
        "        # Define some bool conditions for labels\n",
        "        first_col = j == 0\n",
        "        last_col = j == max_predict_domains - 1\n",
        "        first_row = k == 0\n",
        "        last_row = k == len(models_to_eval) - 1\n",
        "        last_plot = last_col and last_row\n",
        "\n",
        "        # Filter our test set as well as our predictions for the current domain\n",
        "        test_of_current_domain_x = ood_ds.x[ood_ds.dist_shift_domain == domain]\n",
        "        test_of_current_domain_y = ood_ds.y[ood_ds.dist_shift_domain == domain]\n",
        "        pred_of_current_domain = pred[ood_ds.dist_shift_domain == domain]\n",
        "\n",
        "        # Display the decision boundary for this domain\n",
        "        disp = plot_decision_boundary(\n",
        "            estimator=model,\n",
        "            all_X=current_dataset.x,\n",
        "            X=test_of_current_domain_x,\n",
        "            y_gt=test_of_current_domain_y,\n",
        "            y_pred=pred_of_current_domain,\n",
        "            dist_shift_domain=domain,\n",
        "            xlabel=\"$x_1$\" if last_row else \"\",\n",
        "            ylabel=\"$x_2$\" if first_col else \"\",\n",
        "            grid_resolution=100,\n",
        "            ax=ax,\n",
        "            eps=0.1,\n",
        "            show_colorbar=last_plot,\n",
        "            show_legend=last_plot,\n",
        "            cbar_ax=cbar_ax if last_plot else None,\n",
        "            legend_ax=legend_ax if last_plot else None,\n",
        "        )\n",
        "\n",
        "        # Set some desc\n",
        "        if first_row:\n",
        "            ax.set_title(f\"Train Domains: 0-{num_train_domains-1} | Test Domain {num_train_domains+j}\", fontsize=20)\n",
        "\n",
        "        if first_col:\n",
        "            ax.annotate(\n",
        "                label,\n",
        "                xy=(0, 0.5),\n",
        "                xytext=(-ax.yaxis.labelpad - 10, 0),\n",
        "                xycoords=ax.yaxis.label,\n",
        "                textcoords=\"offset points\",\n",
        "                size=20,\n",
        "                ha=\"right\",\n",
        "                va=\"center\",\n",
        "                rotation=90,\n",
        "            )\n",
        "\n",
        "        ax.tick_params(\n",
        "            labelbottom=False, labelleft=False, labelright=False, labeltop=False\n",
        "        )\n",
        "\n",
        "\n",
        "for k, (name, model) in enumerate(models_to_eval.items()):\n",
        "    print(f\"Evaluating {name}...\")\n",
        "    process_model(model, name_mapping.get(name, name), k)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mW4J7Z_Oh_Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative Analysis\n",
        "\n",
        "This section allows to reproduce the quantitative results stated in our paper."
      ],
      "metadata": {
        "id": "IxdSrFVpu67d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Common Setup\n",
        "\n",
        "Execute this cell before moving on to the sections below.\n"
      ],
      "metadata": {
        "id": "UeFNUrWa4qLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import pandas as pd\n",
        "\n",
        "from tabpfn.datasets import get_benchmark_for_task\n",
        "from tabpfn.utils import default_task_settings\n",
        "\n",
        "max_samples, max_features, max_times, max_classes = default_task_settings()\n",
        "\n",
        "task_type = \"dist_shift_multiclass\"\n",
        "\n",
        "datasets_dict = {}\n",
        "\n",
        "# In case the dataset folktables can't be loaded from US census with a certificate error, activate the following context manager to ignore it:\n",
        "with temporary_no_ssl_verify():\n",
        "# with nullcontext():\n",
        "# Load the validation and test datasets.\n",
        "    for split in [\"valid\", \"test\"]:\n",
        "        datasets, _ = get_benchmark_for_task(\n",
        "            task_type=task_type,\n",
        "            split=split,\n",
        "            max_samples=max_samples,\n",
        "            max_features=max_features,\n",
        "            max_classes=max_classes,\n",
        "            return_as_lists=False,\n",
        "        )\n",
        "\n",
        "        datasets_dict[split] = datasets\n",
        "\n",
        "\n",
        "# Define the evaluation parameters for the different evaluation scenarios used\n",
        "# in our paper.\n",
        "def get_eval_kwargs(setting):\n",
        "    if setting == \"w_indices\":\n",
        "        return {\n",
        "            \"minimize_num_train_domains\": False,\n",
        "            \"append_domain_as_feature\": True,\n",
        "        }\n",
        "    elif setting == \"wo_indices\":\n",
        "        return {\n",
        "            \"minimize_num_train_domains\": False,\n",
        "            \"append_domain_as_feature\": False,\n",
        "        }\n",
        "    elif setting == \"l_dom_wo_indices\":\n",
        "        return {\n",
        "            \"minimize_num_train_domains\": True,\n",
        "            \"append_domain_as_feature\": False,\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown setting: {setting}\")\n",
        "\n",
        "\n",
        "# Helper function to simplify metric names\n",
        "def simplify_name(metric, includes_model_type=False):\n",
        "    parts = metric.split(\"/\")\n",
        "    offset = 1 if includes_model_type else 0\n",
        "\n",
        "    # Define metric's domain and name\n",
        "    domain = parts[1 + offset]\n",
        "    metric_name = parts[-1].replace(\"mean_\", \"\")\n",
        "\n",
        "    # Determine benchmark based on parts, handling possible special cases\n",
        "    benchmark = \"overall\"\n",
        "    if len(parts) >= 5 + offset:\n",
        "        benchmark_part = parts[4 + offset]\n",
        "        if \"real-world\" in benchmark_part or \"synthetic\" in benchmark_part:\n",
        "            benchmark = benchmark_part.split(\"_\")[1]\n",
        "        elif \"dataset\" in parts[3 + offset]:\n",
        "            benchmark = parts[4 + offset].split(\"_\")[0].replace(\" \", \"_\")\n",
        "\n",
        "    return f\"{domain}/{benchmark}/{metric_name}\"\n",
        "\n",
        "\n",
        "# Helper function to filter the evaluation results\n",
        "def generate_vis_metrics(split):\n",
        "    domains = [\"ood\", \"id\"]\n",
        "    portions = [\"\", \"/per_task/benchmark_real-world\", \"/per_task/benchmark_synthetic\"]\n",
        "    metrics = [\"mean_acc\", \"mean_f1\", \"mean_roc\",  \"mean_ece\"]\n",
        "\n",
        "    vis_metrics = [\n",
        "        f\"{split}/{domain}/3_splits{task}/{metric}\"\n",
        "        for task, metric, domain in product(portions, metrics, domains)\n",
        "    ]\n",
        "    simplified_metrics = [simplify_name(metric) for metric in vis_metrics]\n",
        "\n",
        "    return vis_metrics, simplified_metrics\n",
        "\n",
        "\n",
        "# Set pandas display options.\n",
        "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 2000)\n",
        "\n",
        "\n",
        "# Helper function to create and sort a pandas dataframe of the results\n",
        "def get_df(results, metrics):\n",
        "    df = pd.DataFrame(\n",
        "        list(results.values()), columns=[\"name\", \"initialization\", \"setting\"] + metrics\n",
        "    )\n",
        "    df.sort_values(by=[\"setting\", \"name\", \"initialization\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Initialize results to be None\n",
        "results = None"
      ],
      "metadata": {
        "id": "lFq89hNmvB5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate TabPFN\n",
        "\n",
        "Evaluate TabPFN$_\\text{dist}$ and TabPFN$_\\text{base}$ on our test datasets to reproduce the performance metrics in our paper.\n",
        "\n",
        "> **NOTE:** Due to ensembling and preprocessing this evaluation took us about 30min per model and setting on 8 CPUs and 1 GPU. Intermediate results are saved in the evaluation folder in the left sidebar. Download and restore them to continue where you left off or distribute these tasks on a cluster you have access to."
      ],
      "metadata": {
        "id": "ubhCj791v71L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from tabpfn.scripts.tabular_evaluation import evaluate_and_score\n",
        "from tabpfn.scripts.tabular_baselines import transformer_metric\n",
        "\n",
        "from tabpfn.scripts.tabular_metrics import (\n",
        "    get_standard_eval_metrics,\n",
        "    get_main_eval_metric,\n",
        ")\n",
        "\n",
        "# Create an accordion widget\n",
        "accordion = widgets.Accordion(\n",
        "    children=[\n",
        "        widgets.Output(),\n",
        "        widgets.Output(layout={\"height\": \"800px\", \"overflow\": \"auto\"}),\n",
        "    ]\n",
        ")\n",
        "accordion.set_title(0, \"Current Results\")\n",
        "accordion.set_title(1, \"Evaluation Output\")\n",
        "display(accordion)\n",
        "\n",
        "# For the TabPFN base models you can define this to be 'w_indices',\n",
        "# 'wo_indices', 'l_dom_wo_indices'. TabPFN dist is implemented to only work\n",
        "# correctly for the setting 'w_indices'.\n",
        "settings = [\"w_indices\", \"wo_indices\", \"l_dom_wo_indices\"]\n",
        "\n",
        "# You can switch between the 'test' and 'valid' datasets here.\n",
        "split = \"test\"\n",
        "\n",
        "# Get result strings to filter for after the evaluation is done.\n",
        "vis_metrics, simplified_metrics = generate_vis_metrics(split)\n",
        "\n",
        "# Start the evaluation.\n",
        "results = {} if results is None else results\n",
        "\n",
        "for setting in settings:\n",
        "    for model_name, model in models.items():\n",
        "        # Skip the settings 'wo_indices' and 'l_dom_wo_indices' for TabPFN dist.\n",
        "        if \"tabpfn_dist\" in model_name and setting != \"w_indices\":\n",
        "            continue\n",
        "\n",
        "        with accordion.children[1]:\n",
        "            print(\n",
        "                f\"Evaluation of {model_name} in setting {setting} on 3 splits of {split} datasets.\"\n",
        "            )\n",
        "\n",
        "            # For our methods we need to extract the model initialization from the filename.\n",
        "            matches = re.match(r\"^(.*?)(_)(\\d+)$\", model_name)\n",
        "            if matches:\n",
        "                model_name, init = matches.group(1), int(matches.group(3))\n",
        "                # init is off by one in comparison to other baselines\n",
        "                init -= 1\n",
        "            else:\n",
        "                init = 1\n",
        "\n",
        "            log_msg, _ = evaluate_and_score(\n",
        "                method_name=model_name,\n",
        "                valid_datasets=datasets_dict[split],\n",
        "                valid_metrics=get_standard_eval_metrics(task_type),\n",
        "                metric_with_model=partial(transformer_metric, classifier=model),\n",
        "                metric_used=get_main_eval_metric(task_type),\n",
        "                split_name=split,\n",
        "                log_per_dataset_metrics=True,\n",
        "                log_per_split_metrics=True,\n",
        "                num_splits=3,\n",
        "                base_path=\"./evaluation/\",\n",
        "                eval_kwargs=get_eval_kwargs(setting),\n",
        "                device=\"cuda\",\n",
        "                rename_gpu_runs=True,\n",
        "                save=True,\n",
        "                overwrite=False,\n",
        "                path_interfix=f\"{setting}_{init}\",\n",
        "            )\n",
        "\n",
        "        # Store the evaluation result.\n",
        "        results[f\"{setting}_{model_name}_{init}\"] = {\n",
        "            **{\"name\": model_name, \"initialization\": init, \"setting\": setting},\n",
        "            **{\n",
        "                simplified_metric: log_msg[metric]\n",
        "                for metric, simplified_metric in zip(vis_metrics, simplified_metrics)\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Print the current state of the results.\n",
        "        with accordion.children[0]:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Updated results: \")\n",
        "            display(get_df(results, simplified_metrics))\n",
        "            accordion.selected_index = 0\n",
        "\n",
        "\n",
        "# Print the current state of the results.\n",
        "with accordion.children[0]:\n",
        "    clear_output(wait=True)\n",
        "    print(\"Evaluation done. Final results: \")\n",
        "    display(get_df(results, simplified_metrics))\n",
        "    accordion.selected_index = 0"
      ],
      "metadata": {
        "id": "PHD8r85ivPOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Baselines\n",
        "\n",
        "Evaluate the tree-based baselines as well as the methods of the WildTime-Benchmark on our test datasets to reproduce the performance metrics in our paper. Note, however, that we used 8 CPUs and 1 GPU for model training. Also, newer versions of the packages for CatBoost, XGBoost, and LightGBM were released since the release of our paper, leading to slight deviations in the results.\n",
        "\n",
        "> **NOTE:** With a `max_time` of 1200s for hpo per dataset split, this will take about 18h per method and setting. Intermediate results are saved in the evaluation folder in the left sidebar. Download and restore them to continue where you left off or distribute these tasks on a cluster you have access to."
      ],
      "metadata": {
        "id": "qFWQ_SAau-lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import pandas as pd\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from tabpfn.scripts.tabular_evaluation import evaluate_and_score\n",
        "from tabpfn.scripts.tabular_baselines import transformer_metric, get_clf_dict\n",
        "\n",
        "from tabpfn.scripts.tabular_metrics import get_standard_eval_metrics, get_main_eval_metric\n",
        "\n",
        "# Create an accordion widget\n",
        "accordion = widgets.Accordion(children=[widgets.Output(), widgets.Output(layout={'height': '800px', 'overflow': 'auto'})])\n",
        "accordion.set_title(0, 'Current Results')\n",
        "accordion.set_title(1, 'Evaluation Output')\n",
        "display(accordion)\n",
        "\n",
        "# For the TabPFN base models you can define this to be 'w_indices',\n",
        "# 'wo_indices', 'l_dom_wo_indices'. TabPFN dist is implemented to only work\n",
        "# correctly for the setting 'w_indices'.\n",
        "settings = [\"w_indices\", \"wo_indices\", \"l_dom_wo_indices\"]\n",
        "\n",
        "# You can switch between the 'test' and 'valid' datasets here.\n",
        "split = \"test\"\n",
        "\n",
        "# The number of model initializations to train and evaluate.\n",
        "num_initializations = 3\n",
        "\n",
        "# The maximum time budget in seconds for hpo of each baseline method on each dataset split.\n",
        "max_time = 1200\n",
        "\n",
        "# Get result strings to filter for after the evaluation is done.\n",
        "vis_metrics, simplified_metrics = generate_vis_metrics(split)\n",
        "\n",
        "# Baseline methods to train and evaluate.\n",
        "tree_methods = [\"catboost\", \"xgb\", \"lightgbm\"]\n",
        "wildtime_methods = ['wildtime_MLP_erm', 'wildtime_MLP_ft', 'wildtime_MLP_ewc',\n",
        "                    'wildtime_MLP_si', 'wildtime_MLP_agem', 'wildtime_MLP_coral',\n",
        "                    'wildtime_MLP_groupdro', 'wildtime_MLP_irm', 'wildtime_MLP_erm_mixup',\n",
        "                    'wildtime_MLP_erm_lisa', 'wildtime_MLP_swa']\n",
        "\n",
        "# Filter here for the methods you like to evaluate.\n",
        "baseline_methods = tree_methods # + wildtime_methods\n",
        "\n",
        "# Start the evaluation.\n",
        "results = {} if results is None else results\n",
        "for init in range(num_initializations):\n",
        "    for setting in settings:\n",
        "        for baseline_method in baseline_methods:\n",
        "            # Skip the setting 'l_dom_wo_indices' for WildTime non-ERM methods.\n",
        "            if baseline_method.startswith(\"wildtime\") and baseline_method != \"wildtime_MLP_erm\" and setting == 'l_dom_wo_indices':\n",
        "                continue\n",
        "\n",
        "            with accordion.children[1]:\n",
        "                print(f\"Training and evaluation {baseline_method} (init {init}) in setting {setting} on 3 splits of {split} datasets.\")\n",
        "\n",
        "                log_msg, _ = evaluate_and_score(\n",
        "                            method_name       = baseline_method,\n",
        "                            valid_datasets    = datasets_dict[split],\n",
        "                            valid_metrics     = get_standard_eval_metrics(task_type),\n",
        "                            metric_with_model = get_clf_dict(task_type)[baseline_method],\n",
        "                            metric_used       = get_main_eval_metric(task_type),\n",
        "                            split_name        = split,\n",
        "                            log_per_dataset_metrics = True,\n",
        "                            log_per_split_metrics   = True,\n",
        "                            num_splits        = 3,\n",
        "                            base_path         = \"./evaluation/\",\n",
        "                            eval_kwargs       = get_eval_kwargs(setting),\n",
        "                            device            = \"cuda\",\n",
        "                            rename_gpu_runs   = True,\n",
        "                            save              = True,\n",
        "                            overwrite         = False,\n",
        "                            path_interfix     = f\"{setting}_{init}\",\n",
        "                            random_state      = init,\n",
        "                            max_time          = max_time\n",
        "                          )\n",
        "\n",
        "            # Store the evaluation result.\n",
        "            results[f\"{setting}_{baseline_method}_{init}\"] = ({\n",
        "                      **{'name': baseline_method, 'initialization': init, 'setting': setting},\n",
        "                      **{simplified_metric: log_msg[metric] for metric, simplified_metric in zip(vis_metrics, simplified_metrics)}\n",
        "                  })\n",
        "\n",
        "            # Print the current state of the results.\n",
        "            with accordion.children[0]:\n",
        "                clear_output(wait=True)\n",
        "                print(\"Updated results: \")\n",
        "                display(get_df(results, simplified_metrics))\n",
        "                accordion.selected_index = 0\n",
        "\n",
        "with accordion.children[0]:\n",
        "    clear_output(wait=True)\n",
        "    print(\"Evaluation done. Final results: \")\n",
        "    display(get_df(results, simplified_metrics))\n",
        "    accordion.selected_index = 0"
      ],
      "metadata": {
        "id": "7laYpJKYwArP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Mean and Confidence Intervals\n",
        "\n",
        "Once the results table is complete, you can calculate the mean value and the confidence intervals for all methods and settings across initializations as follows."
      ],
      "metadata": {
        "id": "j_CFFQZBOD4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as st\n",
        "\n",
        "def compute_mean_and_conf_interval(accuracies, confidence=.95):\n",
        "    accuracies = np.array(accuracies)\n",
        "    n = len(accuracies)\n",
        "\n",
        "    # Only show results if we have 3 initializations for each method\n",
        "    if n != 3:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    m, se = np.mean(accuracies), st.sem(accuracies)\n",
        "    h = se * st.t.ppf((1 + confidence) / 2., n - 1) if n > 1 else 0\n",
        "\n",
        "    return m, h\n",
        "\n",
        "# Define a dictionary of aggregation functions for each metric\n",
        "agg_dict = {metric: [(\"mean\", lambda x: compute_mean_and_conf_interval(x)[0]),\n",
        "                      (\"ci\", lambda x: compute_mean_and_conf_interval(x)[1])]\n",
        "            for metric in simplified_metrics}\n",
        "\n",
        "# Get the results per method and initialization\n",
        "results_per_initialization = get_df(results, simplified_metrics)\n",
        "\n",
        "# Aggregate accross initializations for each method and setting\n",
        "result_df = results_per_initialization.groupby(['name', 'setting']).agg(agg_dict)\n",
        "\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "qHkRx24YOJLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}